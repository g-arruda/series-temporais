# Introdução

## Regressão linear
Regressão linear é uma ferramenta estatística usada para modelar a relação entre uma variável dependente e uma ou mais variáveis independentes, assumindo que essa relação pode ser descrita por uma linha reta. A ideia de se utilizar é uma é dado a sua simplicidade, tendo apenas um parâmetro de inclinação e um de intercepto, uma outra é que aqui se assume que as variáveis apresentam uma relação linear. A linha representa a melhor aproximação da tendência central dos dados. Aqui devemos partir de uma amostra, um par ordenado $\{x_{i},y_{i}\}^{N}_{i=1}$, encontrar uma reta que melhor se ajusta a média dos dados, para isso, vamos partir da equação de uma reta.
$$
y=\alpha + \beta x
$$

Onde a ideia aqui é querer entender qual relação em que a variável $x$ afeta a variável $y$, temos então que resolver dois problemas: primeiro é encontrar os parâmetros $\alpha$ e $\beta$ que melhor se ajusta, sabendo que nem todo o $y$ pode ser explicado pelo $x$, temos que adicionar uma variável à equação que consiga captar essa relação no modelo, essa variável será dada por $u$.

Podemos reescrever a equação acima como sendo um sistema de equações lineares
$$
\begin{aligned}
  y_{1} &= \alpha + \beta x_{1} + u_{1} \\
  y_{2} &= \alpha + \beta x_{2} + u_{2} \\
  y_{3} &= \alpha + \beta x_{3} + u_{3} \\
  & \vdots \\
  y_{n} &= \alpha + \beta x_{n} + u_{n}
\end{aligned}
$$

Note que esse é um sistema de $n$ equações lineares com $n + 2$ incógnitas. E que pela regra de Cramer, sabemos que o sistema apresenta infinitas soluções. O que não nos ajuda e precisamos voltar ao problema, quais valores de $\alpha$ e $\beta$ que melhor se ajusta? Uma maneira de se fazer isso, é minimizar a soma do erro quadrático $\left(\sum_{i=1}^{N} u_i^{2}\right)$ e para isso, vamos isolar o erro, elevar tudo ao quadrado e aplicar a recursividade.
$$
\sum_{i=1}^{N} u_i^{2} = \sum_{i=1}^{N} (y_{i} - \alpha - \beta x_{i})^{2}
$$

Dado isso, podemos dizer que podemos estimar valores de $\alpha$ e $\beta$ que minimizam o erro quadrático. Seja $S(\alpha, \beta) = \sum_{i=1}^{N} u_i^{2}$ e sabendo que os valores dos parâmetros que zeram o gradiente $\nabla = \left(\frac{\partial S}{\partial \hat{\alpha}}, \frac{\partial S}{\partial \hat{\beta}}\right)=0$ são os valores que minimizam o erro quadrático. Fazendo as derivadas...
$$
\begin{aligned}
  \nabla = \begin{bmatrix}
    \dfrac{\partial S}{\partial \hat{\alpha}} \\
    \dfrac{\partial S}{\partial \hat{\beta}}
  \end{bmatrix} = \begin{bmatrix}
    -2\sum_{i=1}^{N} (y_{i} - \hat{\alpha} - \hat{\beta} x_{i}) \\
    -2\sum_{i=1}^{N} (y_{i} - \hat{\alpha} - \hat{\beta} x_{i})(x_{i})
  \end{bmatrix} = 0
\end{aligned}
$$

Podemos multiplicar ambos os lados por $-\frac{1}{2}$ e abrir o somatório^[Note que se somarmos n vezes um parâmetro é o mesmo que dizer n vezes o parâmetro, logo $\sum_{i=1}^{N}\hat{\alpha} = n\hat{\alpha}$.].
$$
\begin{aligned}
  \begin{bmatrix}
    \sum_{i=1}^{N} y_{i} - n\hat{\alpha} - \hat{\beta}\sum_{i=1}^{N} x_{i} \\
    \sum_{i=1}^{N} y_{i}x_{i} - \hat{\alpha}\sum_{i=1}^{N}x_{i} - \hat{\beta}\sum_{i=1}^{N} x_{i}^{2}
  \end{bmatrix} = 0
\end{aligned}
$$

Separando os termos, temos que
$$
\begin{aligned}
  \begin{bmatrix}
    \sum_{i=1}^{N} y_{i} \\
    \sum_{i=1}^{N} y_{i}x_{i}
  \end{bmatrix} =
  \begin{bmatrix}
    n\hat{\alpha} + \hat{\beta}\sum_{i=1}^{N} x_{i} \\
    \hat{\alpha}\sum_{i=1}^{N}x_{i} + \hat{\beta}\sum_{i=1}^{N} x_{i}^{2}
  \end{bmatrix}
\end{aligned}
$$
$$
\begin{aligned}
  \begin{bmatrix}
    \sum_{i=1}^{N} y_{i} \\
    \sum_{i=1}^{N} y_{i}x_{i}
  \end{bmatrix} =
  \begin{bmatrix}
    n & \sum_{i=1}^{N} x_{i} \\
    \sum_{i=1}^{N} x_{i} & \sum_{i=1}^{N} x_{i}^{2}
  \end{bmatrix}
  \begin{bmatrix}
    \hat{\alpha} \\
    \hat{\beta}
  \end{bmatrix}
\end{aligned}
$$

Podemos reorganizar da seguinte maneira:
$$
\begin{bmatrix}
n & \sum_{i=1}^{N} x_{i} \\
\sum_{i=1}^{N} x_{i} & \sum_{i=1}^{N} x_{i}^{2}
\end{bmatrix}
\begin{bmatrix}
\hat{\alpha} \\
\hat{\beta}
\end{bmatrix}
=
\begin{bmatrix}
\sum_{i=1}^{N} y_{i} \\
\sum_{i=1}^{N} y_{i}x_{i}
\end{bmatrix}
$$

Pré-multiplicando ambos os lados pelo inverso da matriz que tem os valores de $x$:
$$
\begin{bmatrix}
\hat{\alpha} \\
\hat{\beta}
\end{bmatrix}
=
\begin{bmatrix}
n & \sum_{i=1}^{N} x_{i} \\
\sum_{i=1}^{N} x_{i} & \sum_{i=1}^{N} x_{i}^{2}
\end{bmatrix}^{-1}
\begin{bmatrix}
\sum_{i=1}^{N} y_{i} \\
\sum_{i=1}^{N} y_{i}x_{i}
\end{bmatrix}
$$

Para termos certeza de que este é o ponto mínimo, devemos avaliar a matriz hessiana:
$$
H =
\begin{bmatrix}
\dfrac{\partial^2 S}{\partial \alpha^2} & \dfrac{\partial^2 S}{\partial \alpha \, \partial \beta} \\
\dfrac{\partial^2 S}{\partial \alpha \, \partial \beta} & \dfrac{\partial^2 S}{\partial \beta^2}
\end{bmatrix}
$$

Logo:
$$
H =
\begin{bmatrix}
n & \sum x_{i} \\
\sum x_{i} & \sum x_{i}^{2}
\end{bmatrix}
$$

Para ser um mínimo global, devemos ter que:

- O primeiro menor principal será $> 0$
- O determinante do segundo menor principal será $>0$

Com isso, podemos dizer que é um ponto de mínimo.

Podemos reescrever:
$$
\begin{bmatrix}
n & \sum x_{i} \\
\sum x_{i} & \sum x_{i}^{2}
\end{bmatrix}
\begin{bmatrix}
\hat{\alpha} \\
\hat{\beta}
\end{bmatrix}
=
\begin{bmatrix}
\sum y_{i} \\
\sum x_{i} y_{i}
\end{bmatrix}
$$

Abrindo:
$$
\begin{bmatrix}
1 & 1 & \dots & 1 \\
x_{1} & x_{2} & \dots & x_{n}
\end{bmatrix}'
\begin{bmatrix}
1 & x_{1} \\
1 & x_{2} \\
\vdots \\
1 & x_{n}
\end{bmatrix}
\begin{bmatrix}
\hat{\alpha} \\
\hat{\beta}
\end{bmatrix}
=
\begin{bmatrix}
y_{1} \\
y_{2} \\
\vdots \\
y_{n}
\end{bmatrix}
$$

Onde:
$$
X =
\begin{bmatrix}
1 & 1 & \dots & 1 \\
x_{1} & x_{2} & \dots & x_{n}
\end{bmatrix}'
$$
$$
\hat{B} =
\begin{bmatrix}
\hat{\alpha} \\
\hat{\beta}
\end{bmatrix}
$$
$$
Y =
\begin{bmatrix}
y_{1} \\
y_{2} \\
\vdots \\
y_{n}
\end{bmatrix}
$$

Então:
$$
X' \hat{B} = X' Y
$$
$$
(X' X) \hat{B} = X' Y
$$
$$
(X' X)^{-1} (X' X) \hat{B} = (X' X)^{-1} X' Y
$$
$$
\hat{B} = (X' X)^{-1} X' Y
$$
$$
\boxed{\hat{B} = (X' X)^{-1} X' Y}
$$

Então, sempre que estamos falando do estimador do **MQO**, estamos nos referindo à fórmula fechada:
$$
\hat{B} = (X' X)^{-1} X' Y
$$

Agora, temos que pensar da seguinte maneira: dado que conseguimos construir os estimadores, como podemos criar seus intervalos de confiança? Para isso, podemos substituir $Y$ por $XB + U$:^[Vale lembrar que $B$ sem chapéu é o melhor ajuste possível da reta, os valores que só Deus sabe.]
$$
\hat{B} = (X' X)^{-1} X' (XB + U)
$$
$$
= (X' X)^{-1} X' XB + (X' X)^{-1} X' U
$$

Assumindo que os dados não tenham problema de multicolinearidade perfeita, a matriz $(X' X)^{-1}$ deve existir para que $I = (X' X)^{-1} X' X$:
$$
\hat{B} = B + (X' X)^{-1} X' U
$${#eq-endo}

Observamos na equação @eq-endo que a componente do estimador influenciada pelo erro, especificamente $X' U$, ilustra uma premissa importante do modelo: $\mathbb{E}(X | U) = 0$. Isso implica que, idealmente, todas as variáveis explicativas deveriam ser exógenas, não apresentando qualquer correlação com o termo de erro. Mas, é importante reconhecer que, na prática, alcançar uma exogeneidade completa é praticamente inviável; assim, é realista esperar que qualquer modelo econômico possa manifestar algum nível, mesmo que mínimo, de endogeneidade.

Subtraind os dois lados da @eq-endo por $-B$ e pós-multiplicando por $(\hat{B} - B)'$:
$$
(\hat{B} - B)(\hat{B} - B)' = (X' X)^{-1} X' U[(X' X)^{-1} X' U]'
$$

Desenvolvendo a parte esquerda dessa igualdade, temos que:
$$
\begin{bmatrix}
\hat{B}_{1} - B \\
\hat{B}_{2} - B
\end{bmatrix}
\begin{bmatrix}
\hat{B}_{1} - B & \hat{B}_{2} - B
\end{bmatrix}'
$$

Multiplicando e aplicando o operador da esperança:
$$
\begin{bmatrix}
\mathbb{E}[(\hat{B}_{1} - B)^{2}] & \mathbb{E}[(\hat{B}_{1} - B)(\hat{B}_{2} - B)] \\
\mathbb{E}[(\hat{B}_{1} - B)(\hat{B}_{2} - B)] & \mathbb{E}[(\hat{B}_{2} - B)^{2}]
\end{bmatrix}
$$

Onde a diagonal principal é a variância de $\hat{B}_{1}$ e o resto é a covariância, então montamos a matriz de variância-covariância:
$$
\begin{bmatrix}
\text{Var}(\hat{B}_{1}) & \text{Cov}(\hat{B}_{1}, \hat{B}_{2}) \\
\text{Cov}(\hat{B}_{1}, \hat{B}_{2}) & \text{Var}(\hat{B}_{2})
\end{bmatrix}
$$

A partir disso, poderíamos montar um intervalo de confiança para os betas se não fosse um pequeno problema... Aqui precisamos do valor de $\beta$, e que só Deus sabe. Vamos então olhar para o lado direito da igualdade^[Vale lembrar que $(AB)' = B' A'$]
$$
=(X' X)^{-1} X' UU' X[(X' X)^{-1}]'
$$
$$
=(X' X)^{-1} X' UU' X(X' X)^{-1}
$$

Abrindo $UU'$ e aplicando o operador da esperança:
$$
UU' =
\begin{bmatrix}
u_1 \\
u_2 \\
\vdots \\
u_n
\end{bmatrix}
\begin{bmatrix}
u_1 & u_2 & \cdots & u_n
\end{bmatrix}'
$$
$$
=
\begin{bmatrix}
\mathbb{E}(u_1)^{2} & \mathbb{E}(u_1, u_2) & \cdots & \mathbb{E}(u_1, u_n) \\
\mathbb{E}(u_2, u_1) & \mathbb{E}(u_2)^{2} & \cdots & \mathbb{E}(u_2, u_n) \\
\vdots & \vdots & \ddots & \vdots \\
\mathbb{E}(u_n, u_1) & \mathbb{E}(u_n, u_2) & \cdots & \mathbb{E}(u_n)^{2}
\end{bmatrix}
$$
Vamos ter que na diagonal principal é a variância dos erros e $\forall \, \mathbb{E}(u_{i}, u_{j})$ em que $i \neq j$ temos a covariância dos erros. Sob as hipóteses de homoscedasticidade^[Variância dos erros é constante, isto é, $\mathbb{E}(u_i^2) = \sigma^2, \, \forall i=1, \ldots, n$] e não autocorrelação, vamos ter que:

$$
UU' = \begin{bmatrix}
\sigma^{2} & 0 & \cdots & 0 \\
0 & \sigma^{2} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma^{2}
\end{bmatrix}
$$

$$
= \sigma^{2} \begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}
$$

$$
= \sigma^{2} I
$$

Então continuando, vamos ter que:

$$
(X' X)^{-1} X' \sigma^{2} X(X' X)^{-1}
$$

$$
= \sigma^{2} (X' X)^{-1} X' X(X' X)^{-1}
$$

$$
= \sigma^{2} (X' X)^{-1}
$$

Agora sim temos uma matriz de variância-covariância, mas percebemos que ao longo do caminho foi necessário fazer algumas hipóteses questionáveis, como a homoscedasticidade e não-autocorrelação. Outro problema dessa matriz de variância-covariância é que nela precisamos da média do erro, mas só Deus sabe o erro... o máximo que podemos fazer é procurar uma estimativa para esse erro, e vamos chamá-lo de **resíduo**. Para diferenciar, o **resíduo** é a parte do modelo que não conseguimos explicar e o **erro** é tudo aquilo que afeta o $Y$, mas não é o $X$.

## Conceitos de Convergência

A ideia aqui é entender o que acontece com a amostra à medida que seu tamanho vai para infinito. Embora isso seja puramente teórico, conseguimos tirar algumas ideias para o caso da amostra finita. As duas ideias principais são:

1. A **lei dos grandes números** diz que a média da amostra $X_n = \frac{1}{n} \sum_{i=1}^n X_i$ **converge em probabilidade** para a expectativa $\mu = \mathbb{E}(X_i)$. Isso significa que $X_n$ está próximo de $\mu$ com alta probabilidade.
2. O **teorema do limite central** diz que $\sqrt{n}(X_n - \mu)$ **converge em distribuição** para uma distribuição Normal. Isso significa que a média da amostra tem aproximadamente uma distribuição Normal para grandes valores de $n$.

::: {#def-converg}

## Convergencia

A sequência\footnote{Lembre-se da ideia de convergência de uma sequência. Dado um $\varepsilon > 0$, dizemos que $x_{k} \to x$ se existir um $k_0$, em que $\forall k \geq k_0 \implies |x_{k} - x| < \varepsilon$} de variáveis aleatórias, $X_{1}, X_{2}, \ldots$, **converge em probabilidade** para uma variável aleatória $X$, se $\forall \varepsilon > 0$,

$$
\lim_{n \to \infty} \mathbb{P}(\left|X_{n} - X\right| \geq \varepsilon) = 0 \quad \text{ou} \quad \lim_{n \to \infty} \mathbb{P}(\left|X_{n} - X\right| < \varepsilon) = 1
$$

Note que se $n \to \infty \implies \left|X_{n} - X\right| \to 0$ e isso quer dizer que no limite, a sequência vai se aproximar muito da variável aleatória.

:::

:::{#thm-lei-grande-numero}

## Teorema da Lei dos Grandes Números - Fraca

Seja $X_{1}, X_{2}, \ldots$, variáveis aleatórias iid com $\mathbb{E}[X_{i}] = \mu$ e $\text{Var}[X_{i}] = \sigma^{2} < \infty$. Defina $\bar{X}_{n} = \frac{1}{n} \sum_{i=1}^{n} X_{i}$. Então para todo $\varepsilon > 0$:

$$
\lim_{n \to \infty} \mathbb{P}(|\bar{X}_{n} - \mu| < \varepsilon) = 1
$$

Então, $\bar{X}_{n}$ **converge em probabilidade** para $\mu$.

:::

```{python}
import numpy as np
import matplotlib.pyplot as plt

def lei_grandes_numeros(pop_mean, pop_std, sample_sizes):
    np.random.seed(0)  # Para reprodutibilidade

    sample_means = [np.random.normal(pop_mean, pop_std, size).mean() for size in sample_sizes]

    return np.array(sample_means)

pop_mean = 10
pop_std = 2
n_simulations = 300
sample_sizes = range(1, n_simulations + 1)

# Gerando os dados
sample_means = lei_grandes_numeros(pop_mean, pop_std,sample_sizes)

# Visualização dos resultados
plt.plot(sample_sizes, sample_means, label='Média amostral')
plt.axhline(y=pop_mean, color='r', linestyle='-', label='Média populacional')
plt.xlabel('Tamanho da Amostra')
plt.ylabel('Média')
plt.title('Demonstração da Lei dos Grandes Números')
plt.show()
```




::: {#thm-limite-central}

## Teorema do Limite Central

Sejam $X_{1}, \ldots, X_{n}$ variáveis aleatórias independentes e identicamente distribuídas com média $\mu$ e variância $\sigma^2$. Seja $X_n = \frac{1}{n} \sum_{i=1}^n X_i$. Então,

$$
Z_n = \frac{X_n - \mu}{\sqrt{\frac{\sigma^2}{n}}} = \frac{\sqrt{n}(X_n - \mu)}{\sigma} 
\xrightarrow[n \to \infty]{} Z
$$

onde $Z$ tem uma distribuição normal padrão. Em outras palavras,

$$
\lim_{n \to \infty} \mathbb{P}(Z_n \leq z) = \Phi(z) = \int_{-\infty}^{z} \frac{1}{\sqrt{2 \pi}} e^{-x^2/2} \, dx.
$$

:::

```{python}
import numpy as np
import matplotlib.pyplot as plt

def tcl(N, pop):
    size_sample = 100
    sample_mean = [
        np.random.choice(pop, size=size_sample, replace=True).mean() for _ in range(N)
    ]
    return np.array(sample_mean)

def plot_tcl(N_values, pop):

    fig, axs = plt.subplots(1, len(N_values), figsize=(15, 5), sharey=True)
    
    for i, N in enumerate(N_values):
        sample_means = tcl(N, pop)
        axs[i].hist(sample_means, bins=30, edgecolor='k', alpha=0.7)
        axs[i].set_title(f'{N} Amostras', fontsize=21)
        axs[i].set_xlabel('Média da Amostra', fontsize=18)
        axs[i].set_ylabel('Frequência', fontsize=18)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

np.random.seed(0)
# Gerar a população
pop = np.random.uniform(size=1000)

# Plota um histograma com diferentes números de amostras
plot_tcl([50, 200, 1000], pop)

```


