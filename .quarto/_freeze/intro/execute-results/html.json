{
  "hash": "89b0f0469ea53cacf4373ced726e574b",
  "result": {
    "engine": "jupyter",
    "markdown": "# Introdução\n\n## Regressão linear\nRegressão linear é uma ferramenta estatística usada para modelar a relação entre uma variável dependente e uma ou mais variáveis independentes, assumindo que essa relação pode ser descrita por uma linha reta. A ideia de se utilizar é uma é dado a sua simplicidade, tendo apenas um parâmetro de inclinação e um de intercepto, uma outra é que aqui se assume que as variáveis apresentam uma relação linear. A linha representa a melhor aproximação da tendência central dos dados. Aqui devemos partir de uma amostra, um par ordenado $\\{x_{i},y_{i}\\}^{N}_{i=1}$, encontrar uma reta que melhor se ajusta a média dos dados, para isso, vamos partir da equação de uma reta.\n$$\ny=\\alpha + \\beta x\n$$\n\nOnde a ideia aqui é querer entender qual relação em que a variável $x$ afeta a variável $y$, temos então que resolver dois problemas: primeiro é encontrar os parâmetros $\\alpha$ e $\\beta$ que melhor se ajusta, sabendo que nem todo o $y$ pode ser explicado pelo $x$, temos que adicionar uma variável à equação que consiga captar essa relação no modelo, essa variável será dada por $u$.\n\nPodemos reescrever a equação acima como sendo um sistema de equações lineares\n$$\n\\begin{aligned}\n  y_{1} &= \\alpha + \\beta x_{1} + u_{1} \\\\\n  y_{2} &= \\alpha + \\beta x_{2} + u_{2} \\\\\n  y_{3} &= \\alpha + \\beta x_{3} + u_{3} \\\\\n  & \\vdots \\\\\n  y_{n} &= \\alpha + \\beta x_{n} + u_{n}\n\\end{aligned}\n$$\n\nNote que esse é um sistema de $n$ equações lineares com $n + 2$ incógnitas. E que pela regra de Cramer, sabemos que o sistema apresenta infinitas soluções. O que não nos ajuda e precisamos voltar ao problema, quais valores de $\\alpha$ e $\\beta$ que melhor se ajusta? Uma maneira de se fazer isso, é minimizar a soma do erro quadrático $\\left(\\sum_{i=1}^{N} u_i^{2}\\right)$ e para isso, vamos isolar o erro, elevar tudo ao quadrado e aplicar a recursividade.\n$$\n\\sum_{i=1}^{N} u_i^{2} = \\sum_{i=1}^{N} (y_{i} - \\alpha - \\beta x_{i})^{2}\n$$\n\nDado isso, podemos dizer que podemos estimar valores de $\\alpha$ e $\\beta$ que minimizam o erro quadrático. Seja $S(\\alpha, \\beta) = \\sum_{i=1}^{N} u_i^{2}$ e sabendo que os valores dos parâmetros que zeram o gradiente $\\nabla = \\left(\\frac{\\partial S}{\\partial \\hat{\\alpha}}, \\frac{\\partial S}{\\partial \\hat{\\beta}}\\right)=0$ são os valores que minimizam o erro quadrático. Fazendo as derivadas...\n$$\n\\begin{aligned}\n  \\nabla = \\begin{bmatrix}\n    \\dfrac{\\partial S}{\\partial \\hat{\\alpha}} \\\\\n    \\dfrac{\\partial S}{\\partial \\hat{\\beta}}\n  \\end{bmatrix} = \\begin{bmatrix}\n    -2\\sum_{i=1}^{N} (y_{i} - \\hat{\\alpha} - \\hat{\\beta} x_{i}) \\\\\n    -2\\sum_{i=1}^{N} (y_{i} - \\hat{\\alpha} - \\hat{\\beta} x_{i})(x_{i})\n  \\end{bmatrix} = 0\n\\end{aligned}\n$$\n\nPodemos multiplicar ambos os lados por $-\\frac{1}{2}$ e abrir o somatório^[Note que se somarmos n vezes um parâmetro é o mesmo que dizer n vezes o parâmetro, logo $\\sum_{i=1}^{N}\\hat{\\alpha} = n\\hat{\\alpha}$.].\n$$\n\\begin{aligned}\n  \\begin{bmatrix}\n    \\sum_{i=1}^{N} y_{i} - n\\hat{\\alpha} - \\hat{\\beta}\\sum_{i=1}^{N} x_{i} \\\\\n    \\sum_{i=1}^{N} y_{i}x_{i} - \\hat{\\alpha}\\sum_{i=1}^{N}x_{i} - \\hat{\\beta}\\sum_{i=1}^{N} x_{i}^{2}\n  \\end{bmatrix} = 0\n\\end{aligned}\n$$\n\nSeparando os termos, temos que\n$$\n\\begin{aligned}\n  \\begin{bmatrix}\n    \\sum_{i=1}^{N} y_{i} \\\\\n    \\sum_{i=1}^{N} y_{i}x_{i}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    n\\hat{\\alpha} + \\hat{\\beta}\\sum_{i=1}^{N} x_{i} \\\\\n    \\hat{\\alpha}\\sum_{i=1}^{N}x_{i} + \\hat{\\beta}\\sum_{i=1}^{N} x_{i}^{2}\n  \\end{bmatrix}\n\\end{aligned}\n$$\n$$\n\\begin{aligned}\n  \\begin{bmatrix}\n    \\sum_{i=1}^{N} y_{i} \\\\\n    \\sum_{i=1}^{N} y_{i}x_{i}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    n & \\sum_{i=1}^{N} x_{i} \\\\\n    \\sum_{i=1}^{N} x_{i} & \\sum_{i=1}^{N} x_{i}^{2}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    \\hat{\\alpha} \\\\\n    \\hat{\\beta}\n  \\end{bmatrix}\n\\end{aligned}\n$$\n\nPodemos reorganizar da seguinte maneira:\n$$\n\\begin{bmatrix}\nn & \\sum_{i=1}^{N} x_{i} \\\\\n\\sum_{i=1}^{N} x_{i} & \\sum_{i=1}^{N} x_{i}^{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\alpha} \\\\\n\\hat{\\beta}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sum_{i=1}^{N} y_{i} \\\\\n\\sum_{i=1}^{N} y_{i}x_{i}\n\\end{bmatrix}\n$$\n\nPré-multiplicando ambos os lados pelo inverso da matriz que tem os valores de $x$:\n$$\n\\begin{bmatrix}\n\\hat{\\alpha} \\\\\n\\hat{\\beta}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nn & \\sum_{i=1}^{N} x_{i} \\\\\n\\sum_{i=1}^{N} x_{i} & \\sum_{i=1}^{N} x_{i}^{2}\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\sum_{i=1}^{N} y_{i} \\\\\n\\sum_{i=1}^{N} y_{i}x_{i}\n\\end{bmatrix}\n$$\n\nPara termos certeza de que este é o ponto mínimo, devemos avaliar a matriz hessiana:\n$$\nH =\n\\begin{bmatrix}\n\\dfrac{\\partial^2 S}{\\partial \\alpha^2} & \\dfrac{\\partial^2 S}{\\partial \\alpha \\, \\partial \\beta} \\\\\n\\dfrac{\\partial^2 S}{\\partial \\alpha \\, \\partial \\beta} & \\dfrac{\\partial^2 S}{\\partial \\beta^2}\n\\end{bmatrix}\n$$\n\nLogo:\n$$\nH =\n\\begin{bmatrix}\nn & \\sum x_{i} \\\\\n\\sum x_{i} & \\sum x_{i}^{2}\n\\end{bmatrix}\n$$\n\nPara ser um mínimo global, devemos ter que:\n\n- O primeiro menor principal será $> 0$\n- O determinante do segundo menor principal será $>0$\n\nCom isso, podemos dizer que é um ponto de mínimo.\n\nPodemos reescrever:\n$$\n\\begin{bmatrix}\nn & \\sum x_{i} \\\\\n\\sum x_{i} & \\sum x_{i}^{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\alpha} \\\\\n\\hat{\\beta}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sum y_{i} \\\\\n\\sum x_{i} y_{i}\n\\end{bmatrix}\n$$\n\nAbrindo:\n$$\n\\begin{bmatrix}\n1 & 1 & \\dots & 1 \\\\\nx_{1} & x_{2} & \\dots & x_{n}\n\\end{bmatrix}'\n\\begin{bmatrix}\n1 & x_{1} \\\\\n1 & x_{2} \\\\\n\\vdots \\\\\n1 & x_{n}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\alpha} \\\\\n\\hat{\\beta}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix}\n$$\n\nOnde:\n$$\nX =\n\\begin{bmatrix}\n1 & 1 & \\dots & 1 \\\\\nx_{1} & x_{2} & \\dots & x_{n}\n\\end{bmatrix}'\n$$\n$$\n\\hat{B} =\n\\begin{bmatrix}\n\\hat{\\alpha} \\\\\n\\hat{\\beta}\n\\end{bmatrix}\n$$\n$$\nY =\n\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix}\n$$\n\nEntão:\n$$\nX' \\hat{B} = X' Y\n$$\n$$\n(X' X) \\hat{B} = X' Y\n$$\n$$\n(X' X)^{-1} (X' X) \\hat{B} = (X' X)^{-1} X' Y\n$$\n$$\n\\hat{B} = (X' X)^{-1} X' Y\n$$\n$$\n\\boxed{\\hat{B} = (X' X)^{-1} X' Y}\n$$\n\nEntão, sempre que estamos falando do estimador do **MQO**, estamos nos referindo à fórmula fechada:\n$$\n\\hat{B} = (X' X)^{-1} X' Y\n$$\n\nAgora, temos que pensar da seguinte maneira: dado que conseguimos construir os estimadores, como podemos criar seus intervalos de confiança? Para isso, podemos substituir $Y$ por $XB + U$:^[Vale lembrar que $B$ sem chapéu é o melhor ajuste possível da reta, os valores que só Deus sabe.]\n$$\n\\hat{B} = (X' X)^{-1} X' (XB + U)\n$$\n$$\n= (X' X)^{-1} X' XB + (X' X)^{-1} X' U\n$$\n\nAssumindo que os dados não tenham problema de multicolinearidade perfeita, a matriz $(X' X)^{-1}$ deve existir para que $I = (X' X)^{-1} X' X$:\n$$\n\\hat{B} = B + (X' X)^{-1} X' U\n$${#eq-endo}\n\nObservamos na equação @eq-endo que a componente do estimador influenciada pelo erro, especificamente $X' U$, ilustra uma premissa importante do modelo: $\\mathbb{E}(X | U) = 0$. Isso implica que, idealmente, todas as variáveis explicativas deveriam ser exógenas, não apresentando qualquer correlação com o termo de erro. Mas, é importante reconhecer que, na prática, alcançar uma exogeneidade completa é praticamente inviável; assim, é realista esperar que qualquer modelo econômico possa manifestar algum nível, mesmo que mínimo, de endogeneidade.\n\nSubtraind os dois lados da @eq-endo por $-B$ e pós-multiplicando por $(\\hat{B} - B)'$:\n$$\n(\\hat{B} - B)(\\hat{B} - B)' = (X' X)^{-1} X' U[(X' X)^{-1} X' U]'\n$$\n\nDesenvolvendo a parte esquerda dessa igualdade, temos que:\n$$\n\\begin{bmatrix}\n\\hat{B}_{1} - B \\\\\n\\hat{B}_{2} - B\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{B}_{1} - B & \\hat{B}_{2} - B\n\\end{bmatrix}'\n$$\n\nMultiplicando e aplicando o operador da esperança:\n$$\n\\begin{bmatrix}\n\\mathbb{E}[(\\hat{B}_{1} - B)^{2}] & \\mathbb{E}[(\\hat{B}_{1} - B)(\\hat{B}_{2} - B)] \\\\\n\\mathbb{E}[(\\hat{B}_{1} - B)(\\hat{B}_{2} - B)] & \\mathbb{E}[(\\hat{B}_{2} - B)^{2}]\n\\end{bmatrix}\n$$\n\nOnde a diagonal principal é a variância de $\\hat{B}_{1}$ e o resto é a covariância, então montamos a matriz de variância-covariância:\n$$\n\\begin{bmatrix}\n\\text{Var}(\\hat{B}_{1}) & \\text{Cov}(\\hat{B}_{1}, \\hat{B}_{2}) \\\\\n\\text{Cov}(\\hat{B}_{1}, \\hat{B}_{2}) & \\text{Var}(\\hat{B}_{2})\n\\end{bmatrix}\n$$\n\nA partir disso, poderíamos montar um intervalo de confiança para os betas se não fosse um pequeno problema... Aqui precisamos do valor de $\\beta$, e que só Deus sabe. Vamos então olhar para o lado direito da igualdade^[Vale lembrar que $(AB)' = B' A'$]\n$$\n=(X' X)^{-1} X' UU' X[(X' X)^{-1}]'\n$$\n$$\n=(X' X)^{-1} X' UU' X(X' X)^{-1}\n$$\n\nAbrindo $UU'$ e aplicando o operador da esperança:\n$$\nUU' =\n\\begin{bmatrix}\nu_1 \\\\\nu_2 \\\\\n\\vdots \\\\\nu_n\n\\end{bmatrix}\n\\begin{bmatrix}\nu_1 & u_2 & \\cdots & u_n\n\\end{bmatrix}'\n$$\n$$\n=\n\\begin{bmatrix}\n\\mathbb{E}(u_1)^{2} & \\mathbb{E}(u_1, u_2) & \\cdots & \\mathbb{E}(u_1, u_n) \\\\\n\\mathbb{E}(u_2, u_1) & \\mathbb{E}(u_2)^{2} & \\cdots & \\mathbb{E}(u_2, u_n) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbb{E}(u_n, u_1) & \\mathbb{E}(u_n, u_2) & \\cdots & \\mathbb{E}(u_n)^{2}\n\\end{bmatrix}\n$$\nVamos ter que na diagonal principal é a variância dos erros e $\\forall \\, \\mathbb{E}(u_{i}, u_{j})$ em que $i \\neq j$ temos a covariância dos erros. Sob as hipóteses de homoscedasticidade^[Variância dos erros é constante, isto é, $\\mathbb{E}(u_i^2) = \\sigma^2, \\, \\forall i=1, \\ldots, n$] e não autocorrelação, vamos ter que:\n\n$$\nUU' = \\begin{bmatrix}\n\\sigma^{2} & 0 & \\cdots & 0 \\\\\n0 & \\sigma^{2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma^{2}\n\\end{bmatrix}\n$$\n\n$$\n= \\sigma^{2} \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n$$\n\n$$\n= \\sigma^{2} I\n$$\n\nEntão continuando, vamos ter que:\n\n$$\n(X' X)^{-1} X' \\sigma^{2} X(X' X)^{-1}\n$$\n\n$$\n= \\sigma^{2} (X' X)^{-1} X' X(X' X)^{-1}\n$$\n\n$$\n= \\sigma^{2} (X' X)^{-1}\n$$\n\nAgora sim temos uma matriz de variância-covariância, mas percebemos que ao longo do caminho foi necessário fazer algumas hipóteses questionáveis, como a homoscedasticidade e não-autocorrelação. Outro problema dessa matriz de variância-covariância é que nela precisamos da média do erro, mas só Deus sabe o erro... o máximo que podemos fazer é procurar uma estimativa para esse erro, e vamos chamá-lo de **resíduo**. Para diferenciar, o **resíduo** é a parte do modelo que não conseguimos explicar e o **erro** é tudo aquilo que afeta o $Y$, mas não é o $X$.\n\n## Conceitos de Convergência\n\nA ideia aqui é entender o que acontece com a amostra à medida que seu tamanho vai para infinito. Embora isso seja puramente teórico, conseguimos tirar algumas ideias para o caso da amostra finita. As duas ideias principais são:\n\n1. A **lei dos grandes números** diz que a média da amostra $X_n = \\frac{1}{n} \\sum_{i=1}^n X_i$ **converge em probabilidade** para a expectativa $\\mu = \\mathbb{E}(X_i)$. Isso significa que $X_n$ está próximo de $\\mu$ com alta probabilidade.\n2. O **teorema do limite central** diz que $\\sqrt{n}(X_n - \\mu)$ **converge em distribuição** para uma distribuição Normal. Isso significa que a média da amostra tem aproximadamente uma distribuição Normal para grandes valores de $n$.\n\n::: {#def-converg}\n\n## Convergencia\n\nA sequência^[Lembre-se da ideia de convergência de uma sequência. Dado um $\\varepsilon > 0$, dizemos que $x_{k} \\to x$ se existir um $k_0$, em que $\\forall k \\geq k_0 \\implies |x_{k} - x| < \\varepsilon$] de variáveis aleatórias, $X_{1}, X_{2}, \\ldots$, **converge em probabilidade** para uma variável aleatória $X$, se $\\forall \\varepsilon > 0$,\n\n$$\n\\lim_{n \\to \\infty} \\mathbb{P}(\\left|X_{n} - X\\right| \\geq \\varepsilon) = 0 \\quad \\text{ou} \\quad \\lim_{n \\to \\infty} \\mathbb{P}(\\left|X_{n} - X\\right| < \\varepsilon) = 1\n$$\n\nNote que se $n \\to \\infty \\implies \\left|X_{n} - X\\right| \\to 0$ e isso quer dizer que no limite, a sequência vai se aproximar muito da variável aleatória.\n\n:::\n\n:::{#thm-lei-grande-numero}\n\n## Teorema da Lei dos Grandes Números - Fraca\n\nSeja $X_{1}, X_{2}, \\ldots$, variáveis aleatórias iid com $\\mathbb{E}[X_{i}] = \\mu$ e $\\text{Var}[X_{i}] = \\sigma^{2} < \\infty$. Defina $\\bar{X}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$. Então para todo $\\varepsilon > 0$:\n\n$$\n\\lim_{n \\to \\infty} \\mathbb{P}(|\\bar{X}_{n} - \\mu| < \\varepsilon) = 1\n$$\n\nEntão, $\\bar{X}_{n}$ **converge em probabilidade** para $\\mu$.\n\n:::\n\n::: {#958e3854 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef lei_grandes_numeros(pop_mean, pop_std, sample_sizes):\n    np.random.seed(0)  # Para reprodutibilidade\n\n    sample_means = [np.random.normal(pop_mean, pop_std, size).mean() for size in sample_sizes]\n\n    return np.array(sample_means)\n\npop_mean = 10\npop_std = 2\nn_simulations = 300\nsample_sizes = range(1, n_simulations + 1)\n\n# Gerando os dados\nsample_means = lei_grandes_numeros(pop_mean, pop_std,sample_sizes)\n\n# Visualização dos resultados\nplt.plot(sample_sizes, sample_means, label='Média amostral')\nplt.axhline(y=pop_mean, color='r', linestyle='-', label='Média populacional')\nplt.xlabel('Tamanho da Amostra')\nplt.ylabel('Média')\nplt.title('Demonstração da Lei dos Grandes Números')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-2-output-1.png){width=585 height=450}\n:::\n:::\n\n\n::: {#thm-limite-central}\n\n## Teorema do Limite Central\n\nSejam $X_{1}, \\ldots, X_{n}$ variáveis aleatórias independentes e identicamente distribuídas com média $\\mu$ e variância $\\sigma^2$. Seja $X_n = \\frac{1}{n} \\sum_{i=1}^n X_i$. Então,\n\n$$\nZ_n = \\frac{X_n - \\mu}{\\sqrt{\\frac{\\sigma^2}{n}}} = \\frac{\\sqrt{n}(X_n - \\mu)}{\\sigma} \n\\xrightarrow[n \\to \\infty]{} Z\n$$\n\nonde $Z$ tem uma distribuição normal padrão. Em outras palavras,\n\n$$\n\\lim_{n \\to \\infty} \\mathbb{P}(Z_n \\leq z) = \\Phi(z) = \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2 \\pi}} e^{-x^2/2} \\, dx.\n$$\n\n:::\n\n::: {#93837e0b .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef tcl(N, pop):\n    size_sample = 100\n    sample_mean = [\n        np.random.choice(pop, size=size_sample, replace=True).mean() for _ in range(N)\n    ]\n    return np.array(sample_mean)\n\ndef plot_tcl(N_values, pop):\n\n    fig, axs = plt.subplots(1, len(N_values), figsize=(15, 5), sharey=True)\n    \n    for i, N in enumerate(N_values):\n        sample_means = tcl(N, pop)\n        axs[i].hist(sample_means, bins=30, edgecolor='k', alpha=0.7)\n        axs[i].set_title(f'{N} Amostras', fontsize=21)\n        axs[i].set_xlabel('Média da Amostra', fontsize=18)\n        axs[i].set_ylabel('Frequência', fontsize=18)\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.show()\n\nnp.random.seed(0)\n# Gerar a população\npop = np.random.uniform(size=1000)\n\n# Plota um histograma com diferentes números de amostras\nplot_tcl([50, 200, 1000], pop)\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-3-output-1.png){width=1428 height=431}\n:::\n:::\n\n\n## Séries de tempo\n\nUma série de tempo é caracterizada por uma sequência de observações tomada sequencialmente no tempo. Podemos chamar essa sequência de observações $ (y_{1},y_{2},y_{3}...) $ como sendo variáveis aleatórias, onde $y = f(t) + \\varepsilon_{t}$, em outras palavras, cada observação é uma função do tempo mais um fator aleatório (ruído branco). Como esse $y$ depende do tempo, vamos ter um valor diferente de $y$ para cada valor de $t=0,1,2,...,T$; $y^{*} = f(t^{*}) + \\varepsilon_{t^{*}}$. Então $y_{1}$ é o valor de $y$ no período 1, $y_{2}$ é o valor de $y$ no período 2 e assim vai. Uma série de tempo pode ser dividida entre um processo determinístico ou estocástico.\n\n### Processo determinístico\n\nÉ um processo que não depende de um termo aleatório e sempre irá ter o mesmo resultado dado um valor inicial. O processo $T_{t} = 1 + 0,1t$ é um processo determinístico, pois não depende de nenhum fator aleatório, estando sempre acompanhado de uma constante (1) e um termo de tendência determinística $(0,1t)$.\n\nPodemos observar essa função melhor, gerando o seguinte gráfico no **R**.\n\n### Processo estocástico\n\nO que diferencia um processo estocástico do determinístico é o fator aleatório, por mais que esse fator seja aleatório ele apresenta uma função de distribuição de probabilidade, por mais que se saiba qual é o valor inicial de uma série, não dá para saber com exatidão o próximo valor, mas podemos atribuir probabilidades a diferentes valores.\n\n#### Ruído Branco\n\nUm ruído branco é uma sequência serial de variáveis aleatórias não correlacionadas com média zero e variância finita e constante, seria aquele erro $(u)$ da primeira parte do curso. Assumimos duas condições importantes para o ruído branco: ele deve ser independente um do outro e deve apresentar uma distribuição normal^[Chamada também de *Distribuição normal gaussiana*.]. Sendo escrito da seguinte forma:\n\n$$\n\\begin{aligned}\n\\varepsilon_{t} &\\sim i.i.d.\\mathcal{N}(0, \\sigma^2) \\\\\n\\varepsilon_{t} &\\sim RB(0, \\sigma^2)\n\\end{aligned}\n$$\n\nO ruído branco assume 3 propriedades:\n\n- $\\mathbb{E}[\\varepsilon_{t}] = \\mathbb{E}[\\varepsilon_{t} \\mid \\varepsilon_{t-1}, \\varepsilon_{t-2}, \\dotsc] = 0$\n- $\\mathbb{E}[\\varepsilon_{t} \\varepsilon_{t-j}] = \\text{cov}[\\varepsilon_{t} \\varepsilon_{t-j}] = 0$\n- $\\text{var}[\\varepsilon_{t}] = \\text{cov}[\\varepsilon_{t} \\varepsilon_{t}] = \\sigma^{2}_{\\varepsilon_{t}}$\n\nAs duas primeiras propriedades dizem respeito à impossibilidade de preditividade e à ausência de autocorrelação. A terceira diz respeito à homocedasticidade, a variância ser constante.\n\nPara visualizar isso, criei uma função em **Python** que gera um conjunto de dados de ruído branco usando a função *np.random.normal* que cria um conjunto aleatório de dados de uma distribuição normal, onde $n$ é o tamanho da amostra desejado. \n\n::: {#e448c741 .cell execution_count=3}\n``` {.python .cell-code}\ndef white_noise(n):\n    np.random.seed(0)\n    white_noise = np.random.normal(0, 1, n)\n    \n    plt.clf()\n    plt.plot(white_noise)\n    plt.axhline(0, color = 'black')\n    plt.show()\n    \nwhite_noise(100)\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-4-output-1.png){width=569 height=411}\n:::\n:::\n\n\n#### Passeio aleatório\n\nEsse tempo passeio aleatório se diz pelo fato de que o valor de uma variável em um determinado período é igual ao seu valor no período passado mais um fator aleatório determinado por $\\varepsilon_{t}$, sendo descrito por:\n\n$$\ny_{t} = y_{t-1} + \\varepsilon_{t}\n$$\n\nVamos supor um valor inicial para $y_{t}$ como sendo $y_{1}$, então:\n\n$$\n\\begin{aligned}\ny_{1} &= y_{0} + \\varepsilon_{1} \\\\\ny_{2} &= y_{1} + \\varepsilon_{2} \\\\\ny_{2} &= y_{0} + \\varepsilon_{1} + \\varepsilon_{2}\n\\end{aligned}\n$$\n\nResolvendo isso recursivamente, temos:\n\n$$\ny_{t} = y_{0} + \\sum_{i=1}^{t} \\varepsilon_{i}\n$$\n\nSe $y_{0} \\sim 0$, podemos então dizer que o passeio aleatório é uma soma acumulada de ruídos brancos:\n\n$$\ny_{t} = \\sum_{i=1}^{t} \\varepsilon_{i}\n$$\n\nPode acontecer o caso do passeio aleatório ter uma constante $\\gamma$ adicionada:\n\n$$\ny_{1} = \\gamma + y_{0} + \\varepsilon_{1}\n$$\n\nLogo, quando $y_{0} \\sim 0$, vamos ter que:\n\n$$\ny_{t} = \\gamma t + \\sum_{i=1}^{t} \\varepsilon_{i}\n$$\n\n::: {#6f620649 .cell execution_count=4}\n``` {.python .cell-code}\ndef random_walk(trend, n):\n    np.random.seed(0)\n    \n    yt = np.zeros(n)\n    epsilon = np.random.normal(0,1,n)\n    for t in np.arange(1,n):\n        yt[t] = t * trend + np.sum(epsilon[:t+1])\n    \n    return yt\n\n\ndef plot_rw(trend_list, t):\n    plt.clf()\n    for trend in trend_list:\n        rw = random_walk(trend, t)\n        plt.plot(rw, label=f'Tendencia = {trend}')\n    \n    plt.legend()\n    plt.show()\n\ntrend_list = [0, 0.2, -0.2]\nplot_rw(trend_list, t=100)\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-5-output-1.png){width=577 height=411}\n:::\n:::\n\n\n### Estacionariedade e Autocorrelação\n\nAntes de entrar no assunto de autocorrelação, devemos ter em mente algumas propriedades referentes ao primeiro momento (esperança) e segundo momento (variância) de um passeio aleatório. O primeiro momento de um passeio aleatório pode ser calculado pela sua média:\n\n$$\n\\mathbb{E}[y_{t}] = \\mathbb{E}[y_{0} + \\sum_{i=1}^{t} \\varepsilon_{i}] = \\mathbb{E}[y_{0}] + \\mathbb{E}[\\sum_{i=1}^{t} \\varepsilon_{i}] = \\mathbb{E}[y_{0}] + 0 = y_{0} = \\mu\n$$\n\nNota-se que a esperança (média) de $y_{t}$ não depende de $t$, sendo uma constante e é por isso que estou chamando de $\\mu$.\n\nO segundo momento é a variância:\n\n$$\n\\text{Var}[y_{t}] = \\mathbb{E}[y_{t}^{2}] = \\mathbb{E}[(y_{t} - \\mathbb{E}[y_{t}])^{2}]\n$$\n$$\n= \\mathbb{E}[y_{t}^{2}] - \\mathbb{E}[y_{0}]^{2}\n$$\n$$\n= \\mathbb{E}[(y_{0} + \\sum \\varepsilon_{i})^{2}]\n$$\n$$\n= \\mathbb{E}[y_{0}^{2} + 2y_{0}\\sum \\varepsilon_{i} + \\sum \\varepsilon_{i}^{2}] - \\mu^{2}\n$$\n$$\n= \\mathbb{E}[y_{0}^{2}] + 2y_{0}\\sum \\mathbb{E}[\\varepsilon_{i}] + \\sum \\mathbb{E}[\\varepsilon_{i}^{2}] - \\mu^{2}\n$$\n$$\n= \\mu^{2} + 0 + \\sum \\sigma^{2} - \\mu^{2} = t \\sigma^{2}\n$$\n\nPodemos observar que a variância de um processo aleatório não é constante, pois vai depender de $t$.\n\nOutra parada importante é definir a covariância para $k$ lags:\n\n$$\n\\text{Cov}[y_{t},y_{t-k}] = \\mathbb{E} \\{ (y_{t} - \\mathbb{E}[y_{t}])(y_{t-k} - \\mathbb{E}[y_{t-k}]) \\} = \\mathbb{E} [(y_{t} - \\mu)(y_{t-k} - \\mu )]\n$$\n\n### Estacionariedade\n\nA hipótese da estacionariedade é um caso particular do **processo estocástico**, no qual se assume que o processo está em um estado de equilíbrio. O caso dos processos estritamente estacionários (estacionariedade forte) é quando **todas** as suas propriedades (momentos) não são afetadas pelo tempo. Como essas condições são muito restritas, na grande maioria das vezes nos referimos a esse processo estocástico como **fracamente estacionário**, ou **covariância-estacionário**, ou **estacionário de segunda ordem**. Isso quer dizer que apenas a média e a variância devem ser constantes e que a covariância deve depender apenas do número de lags $(k)$:\n\n$$\n\\mathbb{E}[y_{t}] = \\mu\n$$\n$$\n\\text{Var}[y_{t}] = \\mathbb{E}[(y_{t} - \\mu)^{2}] = \\sigma^2\n$$\n$$\n\\text{Cov}[y_{t},y_{t+k}] = \\mathbb{E} [ (y_{t} - \\mu)(y_{t-k} - \\mu)] = \\gamma_{k}\n$$\n\n### Função de autocorrelação (FAC)\n\nAo assumir a hipótese da estacionariedade, vamos ter que a função conjunta de probabilidade de $y_{t_{1}}$ e $y_{t_{2}}$ será a mesma para todo o tempo $t_{1}$ e $t_{2}$, que será constante. Isso implica que a **covariância** entre $y_{t}$ e $y_{t+k}$ será separada apenas por $k$ intervalos de tempo (lag). Assim, a *autocovariância* ao lag $k$ é definida por:\n\n$$\n\\gamma_{k} = \\text{Cov}[y_{t},y_{t+k}] = \\mathbb{E} [ (y_{t} - \\mu)(y_{t-k} - \\mu)]\n$$\n\nDa mesma forma, teremos que a **autocorrelação** é dada por:\n\n$$\n\\rho_{k} = \\dfrac{\\mathbb{E} [ (y_{t} - \\mu)(y_{t-k} - \\mu)]}{\\sqrt{\\mathbb{E}[(y_{t} - \\mu)^{2}] \\mathbb{E}[(y_{t} - \\mu)^{2}] }} = \\dfrac{\\mathbb{E} [ (y_{t} - \\mu)(y_{t-k} - \\mu)]}{\\sigma^{2}}\n$$\n\nComo em um processo estacionário a variância é constante, vamos ter que $\\gamma_{0} = \\sigma^{2}$. Podemos então dizer que a autocorrelação no lag $k$ é:\n\n$$\n\\rho_{k} = \\dfrac{\\gamma_{k}}{\\gamma_{0}}\n$$\n\nE isso implica que $\\rho_{0} = 1$ se $k=0$.\n\nA **Função de autocorrelação** é quando se plota um gráfico $\\rho_{k}$ contra o lag $k$.\n\n",
    "supporting": [
      "intro_files"
    ],
    "filters": [],
    "includes": {}
  }
}