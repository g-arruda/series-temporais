{
  "hash": "1820cda64fd6895eba57d366a9ea874f",
  "result": {
    "engine": "jupyter",
    "markdown": "# Introdução\n\n## Regressão linear\nRegressão linear é uma ferramenta estatística usada para modelar a relação entre uma variável dependente e uma ou mais variáveis independentes, assumindo que essa relação pode ser descrita por uma linha reta. A ideia de se utilizar é uma é dado a sua simplicidade, tendo apenas um parâmetro de inclinação e um de intercepto, uma outra é que aqui se assume que as variáveis apresentam uma relação linear. A linha representa a melhor aproximação da tendência central dos dados. Aqui devemos partir de uma amostra, um par ordenado $\\{x_{i},y_{i}\\}^{N}_{i=1}$, encontrar uma reta que melhor se ajusta a média dos dados, para isso, vamos partir da equação de uma reta.\n$$\ny=\\alpha + \\beta x\n$$\n\nOnde a ideia aqui é querer entender qual relação em que a variável $x$ afeta a variável $y$, temos então que resolver dois problemas: primeiro é encontrar os parâmetros $\\alpha$ e $\\beta$ que melhor se ajusta, sabendo que nem todo o $y$ pode ser explicado pelo $x$, temos que adicionar uma variável à equação que consiga captar essa relação no modelo, essa variável será dada por $u$.\n\nPodemos reescrever a equação acima como sendo um sistema de equações lineares\n$$\n\\begin{aligned}\n  y_{1} &= \\alpha + \\beta x_{1} + u_{1} \\\\\n  y_{2} &= \\alpha + \\beta x_{2} + u_{2} \\\\\n  y_{3} &= \\alpha + \\beta x_{3} + u_{3} \\\\\n  & \\vdots \\\\\n  y_{n} &= \\alpha + \\beta x_{n} + u_{n}\n\\end{aligned}\n$$\n\nNote que esse é um sistema de $n$ equações lineares com $n + 2$ incógnitas. E que pela regra de Cramer, sabemos que o sistema apresenta infinitas soluções. O que não nos ajuda e precisamos voltar ao problema, quais valores de $\\alpha$ e $\\beta$ que melhor se ajusta? Uma maneira de se fazer isso, é minimizar a soma do erro quadrático $\\left(\\sum_{i=1}^{N} u_i^{2}\\right)$ e para isso, vamos isolar o erro, elevar tudo ao quadrado e aplicar a recursividade.\n$$\n\\sum_{i=1}^{N} u_i^{2} = \\sum_{i=1}^{N} (y_{i} - \\alpha - \\beta x_{i})^{2}\n$$\n\nDado isso, podemos dizer que podemos estimar valores de $\\alpha$ e $\\beta$ que minimizam o erro quadrático. Seja $S(\\alpha, \\beta) = \\sum_{i=1}^{N} u_i^{2}$ e sabendo que os valores dos parâmetros que zeram o gradiente $\\nabla = \\left(\\frac{\\partial S}{\\partial \\hat{\\alpha}}, \\frac{\\partial S}{\\partial \\hat{\\beta}}\\right)=0$ são os valores que minimizam o erro quadrático. Fazendo as derivadas...\n$$\n\\begin{aligned}\n  \\nabla = \\begin{bmatrix}\n    \\dfrac{\\partial S}{\\partial \\hat{\\alpha}} \\\\\n    \\dfrac{\\partial S}{\\partial \\hat{\\beta}}\n  \\end{bmatrix} = \\begin{bmatrix}\n    -2\\sum_{i=1}^{N} (y_{i} - \\hat{\\alpha} - \\hat{\\beta} x_{i}) \\\\\n    -2\\sum_{i=1}^{N} (y_{i} - \\hat{\\alpha} - \\hat{\\beta} x_{i})(x_{i})\n  \\end{bmatrix} = 0\n\\end{aligned}\n$$\n\nPodemos multiplicar ambos os lados por $-\\frac{1}{2}$ e abrir o somatório^[Note que se somarmos n vezes um parâmetro é o mesmo que dizer n vezes o parâmetro, logo $\\sum_{i=1}^{N}\\hat{\\alpha} = n\\hat{\\alpha}$.].\n$$\n\\begin{aligned}\n  \\begin{bmatrix}\n    \\sum_{i=1}^{N} y_{i} - n\\hat{\\alpha} - \\hat{\\beta}\\sum_{i=1}^{N} x_{i} \\\\\n    \\sum_{i=1}^{N} y_{i}x_{i} - \\hat{\\alpha}\\sum_{i=1}^{N}x_{i} - \\hat{\\beta}\\sum_{i=1}^{N} x_{i}^{2}\n  \\end{bmatrix} = 0\n\\end{aligned}\n$$\n\nSeparando os termos, temos que\n$$\n\\begin{aligned}\n  \\begin{bmatrix}\n    \\sum_{i=1}^{N} y_{i} \\\\\n    \\sum_{i=1}^{N} y_{i}x_{i}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    n\\hat{\\alpha} + \\hat{\\beta}\\sum_{i=1}^{N} x_{i} \\\\\n    \\hat{\\alpha}\\sum_{i=1}^{N}x_{i} + \\hat{\\beta}\\sum_{i=1}^{N} x_{i}^{2}\n  \\end{bmatrix}\n\\end{aligned}\n$$\n$$\n\\begin{aligned}\n  \\begin{bmatrix}\n    \\sum_{i=1}^{N} y_{i} \\\\\n    \\sum_{i=1}^{N} y_{i}x_{i}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    n & \\sum_{i=1}^{N} x_{i} \\\\\n    \\sum_{i=1}^{N} x_{i} & \\sum_{i=1}^{N} x_{i}^{2}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    \\hat{\\alpha} \\\\\n    \\hat{\\beta}\n  \\end{bmatrix}\n\\end{aligned}\n$$\n\nPodemos reorganizar da seguinte maneira:\n$$\n\\begin{bmatrix}\nn & \\sum_{i=1}^{N} x_{i} \\\\\n\\sum_{i=1}^{N} x_{i} & \\sum_{i=1}^{N} x_{i}^{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\alpha} \\\\\n\\hat{\\beta}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sum_{i=1}^{N} y_{i} \\\\\n\\sum_{i=1}^{N} y_{i}x_{i}\n\\end{bmatrix}\n$$\n\nPré-multiplicando ambos os lados pelo inverso da matriz que tem os valores de $x$:\n$$\n\\begin{bmatrix}\n\\hat{\\alpha} \\\\\n\\hat{\\beta}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nn & \\sum_{i=1}^{N} x_{i} \\\\\n\\sum_{i=1}^{N} x_{i} & \\sum_{i=1}^{N} x_{i}^{2}\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\sum_{i=1}^{N} y_{i} \\\\\n\\sum_{i=1}^{N} y_{i}x_{i}\n\\end{bmatrix}\n$$\n\nPara termos certeza de que este é o ponto mínimo, devemos avaliar a matriz hessiana:\n$$\nH =\n\\begin{bmatrix}\n\\dfrac{\\partial^2 S}{\\partial \\alpha^2} & \\dfrac{\\partial^2 S}{\\partial \\alpha \\, \\partial \\beta} \\\\\n\\dfrac{\\partial^2 S}{\\partial \\alpha \\, \\partial \\beta} & \\dfrac{\\partial^2 S}{\\partial \\beta^2}\n\\end{bmatrix}\n$$\n\nLogo:\n$$\nH =\n\\begin{bmatrix}\nn & \\sum x_{i} \\\\\n\\sum x_{i} & \\sum x_{i}^{2}\n\\end{bmatrix}\n$$\n\nPara ser um mínimo global, devemos ter que:\n\n- O primeiro menor principal será $> 0$\n- O determinante do segundo menor principal será $>0$\n\nCom isso, podemos dizer que é um ponto de mínimo.\n\nPodemos reescrever:\n$$\n\\begin{bmatrix}\nn & \\sum x_{i} \\\\\n\\sum x_{i} & \\sum x_{i}^{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\alpha} \\\\\n\\hat{\\beta}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sum y_{i} \\\\\n\\sum x_{i} y_{i}\n\\end{bmatrix}\n$$\n\nAbrindo:\n$$\n\\begin{bmatrix}\n1 & 1 & \\dots & 1 \\\\\nx_{1} & x_{2} & \\dots & x_{n}\n\\end{bmatrix}'\n\\begin{bmatrix}\n1 & x_{1} \\\\\n1 & x_{2} \\\\\n\\vdots \\\\\n1 & x_{n}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{\\alpha} \\\\\n\\hat{\\beta}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix}\n$$\n\nOnde:\n$$\nX =\n\\begin{bmatrix}\n1 & 1 & \\dots & 1 \\\\\nx_{1} & x_{2} & \\dots & x_{n}\n\\end{bmatrix}'\n$$\n$$\n\\hat{B} =\n\\begin{bmatrix}\n\\hat{\\alpha} \\\\\n\\hat{\\beta}\n\\end{bmatrix}\n$$\n$$\nY =\n\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{n}\n\\end{bmatrix}\n$$\n\nEntão:\n$$\nX' \\hat{B} = X' Y\n$$\n$$\n(X' X) \\hat{B} = X' Y\n$$\n$$\n(X' X)^{-1} (X' X) \\hat{B} = (X' X)^{-1} X' Y\n$$\n$$\n\\hat{B} = (X' X)^{-1} X' Y\n$$\n$$\n\\boxed{\\hat{B} = (X' X)^{-1} X' Y}\n$$\n\nEntão, sempre que estamos falando do estimador do **MQO**, estamos nos referindo à fórmula fechada:\n$$\n\\hat{B} = (X' X)^{-1} X' Y\n$$\n\nAgora, temos que pensar da seguinte maneira: dado que conseguimos construir os estimadores, como podemos criar seus intervalos de confiança? Para isso, podemos substituir $Y$ por $XB + U$:^[Vale lembrar que $B$ sem chapéu é o melhor ajuste possível da reta, os valores que só Deus sabe.]\n$$\n\\hat{B} = (X' X)^{-1} X' (XB + U)\n$$\n$$\n= (X' X)^{-1} X' XB + (X' X)^{-1} X' U\n$$\n\nAssumindo que os dados não tenham problema de multicolinearidade perfeita, a matriz $(X' X)^{-1}$ deve existir para que $I = (X' X)^{-1} X' X$:\n$$\n\\hat{B} = B + (X' X)^{-1} X' U\n$${#eq-endo}\n\nObservamos na equação @eq-endo que a componente do estimador influenciada pelo erro, especificamente $X' U$, ilustra uma premissa importante do modelo: $\\mathbb{E}(X | U) = 0$. Isso implica que, idealmente, todas as variáveis explicativas deveriam ser exógenas, não apresentando qualquer correlação com o termo de erro. Mas, é importante reconhecer que, na prática, alcançar uma exogeneidade completa é praticamente inviável; assim, é realista esperar que qualquer modelo econômico possa manifestar algum nível, mesmo que mínimo, de endogeneidade.\n\nSubtraind os dois lados da @eq-endo por $-B$ e pós-multiplicando por $(\\hat{B} - B)'$:\n$$\n(\\hat{B} - B)(\\hat{B} - B)' = (X' X)^{-1} X' U[(X' X)^{-1} X' U]'\n$$\n\nDesenvolvendo a parte esquerda dessa igualdade, temos que:\n$$\n\\begin{bmatrix}\n\\hat{B}_{1} - B \\\\\n\\hat{B}_{2} - B\n\\end{bmatrix}\n\\begin{bmatrix}\n\\hat{B}_{1} - B & \\hat{B}_{2} - B\n\\end{bmatrix}'\n$$\n\nMultiplicando e aplicando o operador da esperança:\n$$\n\\begin{bmatrix}\n\\mathbb{E}[(\\hat{B}_{1} - B)^{2}] & \\mathbb{E}[(\\hat{B}_{1} - B)(\\hat{B}_{2} - B)] \\\\\n\\mathbb{E}[(\\hat{B}_{1} - B)(\\hat{B}_{2} - B)] & \\mathbb{E}[(\\hat{B}_{2} - B)^{2}]\n\\end{bmatrix}\n$$\n\nOnde a diagonal principal é a variância de $\\hat{B}_{1}$ e o resto é a covariância, então montamos a matriz de variância-covariância:\n$$\n\\begin{bmatrix}\n\\text{Var}(\\hat{B}_{1}) & \\text{Cov}(\\hat{B}_{1}, \\hat{B}_{2}) \\\\\n\\text{Cov}(\\hat{B}_{1}, \\hat{B}_{2}) & \\text{Var}(\\hat{B}_{2})\n\\end{bmatrix}\n$$\n\nA partir disso, poderíamos montar um intervalo de confiança para os betas se não fosse um pequeno problema... Aqui precisamos do valor de $\\beta$, e que só Deus sabe. Vamos então olhar para o lado direito da igualdade^[Vale lembrar que $(AB)' = B' A'$]\n$$\n=(X' X)^{-1} X' UU' X[(X' X)^{-1}]'\n$$\n$$\n=(X' X)^{-1} X' UU' X(X' X)^{-1}\n$$\n\nAbrindo $UU'$ e aplicando o operador da esperança:\n$$\nUU' =\n\\begin{bmatrix}\nu_1 \\\\\nu_2 \\\\\n\\vdots \\\\\nu_n\n\\end{bmatrix}\n\\begin{bmatrix}\nu_1 & u_2 & \\cdots & u_n\n\\end{bmatrix}'\n$$\n$$\n=\n\\begin{bmatrix}\n\\mathbb{E}(u_1)^{2} & \\mathbb{E}(u_1, u_2) & \\cdots & \\mathbb{E}(u_1, u_n) \\\\\n\\mathbb{E}(u_2, u_1) & \\mathbb{E}(u_2)^{2} & \\cdots & \\mathbb{E}(u_2, u_n) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbb{E}(u_n, u_1) & \\mathbb{E}(u_n, u_2) & \\cdots & \\mathbb{E}(u_n)^{2}\n\\end{bmatrix}\n$$\nVamos ter que na diagonal principal é a variância dos erros e $\\forall \\, \\mathbb{E}(u_{i}, u_{j})$ em que $i \\neq j$ temos a covariância dos erros. Sob as hipóteses de homoscedasticidade^[Variância dos erros é constante, isto é, $\\mathbb{E}(u_i^2) = \\sigma^2, \\, \\forall i=1, \\ldots, n$] e não autocorrelação, vamos ter que:\n\n$$\nUU' = \\begin{bmatrix}\n\\sigma^{2} & 0 & \\cdots & 0 \\\\\n0 & \\sigma^{2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma^{2}\n\\end{bmatrix}\n$$\n\n$$\n= \\sigma^{2} \\begin{bmatrix}\n1 & 0 & \\cdots & 0 \\\\\n0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 1\n\\end{bmatrix}\n$$\n\n$$\n= \\sigma^{2} I\n$$\n\nEntão continuando, vamos ter que:\n\n$$\n(X' X)^{-1} X' \\sigma^{2} X(X' X)^{-1}\n$$\n\n$$\n= \\sigma^{2} (X' X)^{-1} X' X(X' X)^{-1}\n$$\n\n$$\n= \\sigma^{2} (X' X)^{-1}\n$$\n\nAgora sim temos uma matriz de variância-covariância, mas percebemos que ao longo do caminho foi necessário fazer algumas hipóteses questionáveis, como a homoscedasticidade e não-autocorrelação. Outro problema dessa matriz de variância-covariância é que nela precisamos da média do erro, mas só Deus sabe o erro... o máximo que podemos fazer é procurar uma estimativa para esse erro, e vamos chamá-lo de **resíduo**. Para diferenciar, o **resíduo** é a parte do modelo que não conseguimos explicar e o **erro** é tudo aquilo que afeta o $Y$, mas não é o $X$.\n\n## Conceitos de Convergência\n\nA ideia aqui é entender o que acontece com a amostra à medida que seu tamanho vai para infinito. Embora isso seja puramente teórico, conseguimos tirar algumas ideias para o caso da amostra finita. As duas ideias principais são:\n\n1. A **lei dos grandes números** diz que a média da amostra $X_n = \\frac{1}{n} \\sum_{i=1}^n X_i$ **converge em probabilidade** para a expectativa $\\mu = \\mathbb{E}(X_i)$. Isso significa que $X_n$ está próximo de $\\mu$ com alta probabilidade.\n2. O **teorema do limite central** diz que $\\sqrt{n}(X_n - \\mu)$ **converge em distribuição** para uma distribuição Normal. Isso significa que a média da amostra tem aproximadamente uma distribuição Normal para grandes valores de $n$.\n\n::: {#def-converg}\n\n## Convergencia\n\nA sequência\\footnote{Lembre-se da ideia de convergência de uma sequência. Dado um $\\varepsilon > 0$, dizemos que $x_{k} \\to x$ se existir um $k_0$, em que $\\forall k \\geq k_0 \\implies |x_{k} - x| < \\varepsilon$} de variáveis aleatórias, $X_{1}, X_{2}, \\ldots$, **converge em probabilidade** para uma variável aleatória $X$, se $\\forall \\varepsilon > 0$,\n\n$$\n\\lim_{n \\to \\infty} \\mathbb{P}(\\left|X_{n} - X\\right| \\geq \\varepsilon) = 0 \\quad \\text{ou} \\quad \\lim_{n \\to \\infty} \\mathbb{P}(\\left|X_{n} - X\\right| < \\varepsilon) = 1\n$$\n\nNote que se $n \\to \\infty \\implies \\left|X_{n} - X\\right| \\to 0$ e isso quer dizer que no limite, a sequência vai se aproximar muito da variável aleatória.\n\n:::\n\n:::{#thm-lei-grande-numero}\n\n## Teorema da Lei dos Grandes Números - Fraca\n\nSeja $X_{1}, X_{2}, \\ldots$, variáveis aleatórias iid com $\\mathbb{E}[X_{i}] = \\mu$ e $\\text{Var}[X_{i}] = \\sigma^{2} < \\infty$. Defina $\\bar{X}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$. Então para todo $\\varepsilon > 0$:\n\n$$\n\\lim_{n \\to \\infty} \\mathbb{P}(|\\bar{X}_{n} - \\mu| < \\varepsilon) = 1\n$$\n\nEntão, $\\bar{X}_{n}$ **converge em probabilidade** para $\\mu$.\n\n:::\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef lei_grandes_numeros(pop_mean, pop_std, sample_sizes):\n    np.random.seed(0)  # Para reprodutibilidade\n\n    sample_means = [np.random.normal(pop_mean, pop_std, size).mean() for size in sample_sizes]\n\n    return np.array(sample_means)\n\npop_mean = 10\npop_std = 2\nn_simulations = 300\nsample_sizes = range(1, n_simulations + 1)\n\n# Gerando os dados\nsample_means = lei_grandes_numeros(pop_mean, pop_std,sample_sizes)\n\n# Visualização dos resultados\nplt.plot(sample_sizes, sample_means, label='Média amostral')\nplt.axhline(y=pop_mean, color='r', linestyle='-', label='Média populacional')\nplt.xlabel('Tamanho da Amostra')\nplt.ylabel('Média')\nplt.title('Demonstração da Lei dos Grandes Números')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-pdf/cell-2-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {#thm-limite-central}\n\n## Teorema do Limite Central\n\nSejam $X_{1}, \\ldots, X_{n}$ variáveis aleatórias independentes e identicamente distribuídas com média $\\mu$ e variância $\\sigma^2$. Seja $X_n = \\frac{1}{n} \\sum_{i=1}^n X_i$. Então,\n\n$$\nZ_n = \\frac{X_n - \\mu}{\\sqrt{\\frac{\\sigma^2}{n}}} = \\frac{\\sqrt{n}(X_n - \\mu)}{\\sigma} \n\\xrightarrow[n \\to \\infty]{} Z\n$$\n\nonde $Z$ tem uma distribuição normal padrão. Em outras palavras,\n\n$$\n\\lim_{n \\to \\infty} \\mathbb{P}(Z_n \\leq z) = \\Phi(z) = \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2 \\pi}} e^{-x^2/2} \\, dx.\n$$\n\n:::\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef tcl(N, pop):\n    size_sample = 100\n    sample_mean = [\n        np.random.choice(pop, size=size_sample, replace=True).mean() for _ in range(N)\n    ]\n    return np.array(sample_mean)\n\ndef plot_tcl(N_values, pop):\n\n    fig, axs = plt.subplots(1, len(N_values), figsize=(15, 5), sharey=True)\n    \n    for i, N in enumerate(N_values):\n        sample_means = tcl(N, pop)\n        axs[i].hist(sample_means, bins=30, edgecolor='k', alpha=0.7)\n        axs[i].set_title(f'{N} Amostras', fontsize=21)\n        axs[i].set_xlabel('Média da Amostra', fontsize=18)\n        axs[i].set_ylabel('Frequência', fontsize=18)\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.show()\n\nnp.random.seed(0)\n# Gerar a população\npop = np.random.uniform(size=1000)\n\n# Plota um histograma com diferentes números de amostras\nplot_tcl([50, 200, 1000], pop)\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-pdf/cell-3-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n",
    "supporting": [
      "intro_files/figure-pdf"
    ],
    "filters": []
  }
}